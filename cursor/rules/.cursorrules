# Cursor Rules — Ultimate Board Game Tracker (No‑Code Edition)

## 0) Mission & Guardrails

- Mission: reliably log in-person sessions, store raw scores + league points, and keep leaderboards accurate over time (never recompute historical points).
- Non-negotiables: correctness over cleverness; persist league points once; explicit tie-breaks; transactional session save; traceable decisions (what template, which point pattern, when).
- When uncertain: state uncertainty, verify, or ask — do not guess.

## 1) Prompting Principles

- Be explicit and structured. State goal, audience, constraints, inputs/outputs, and success criteria.
- Set a role in the system message (e.g., “backend reviewer focused on data integrity”); keep task-specific instructions in the user turn.
- Use XML-style sections to separate context, instructions, examples, and final answer.
- Encourage guided reasoning for non-trivial tasks (scoring edge cases, tie policies, migrations). Require a short “Final Answer” section.
- Chain complex tasks into subtasks and pass outputs between steps with clear tags.
- For tricky, reusable prompts (code review checklists, scoring verifications), iterate with a prompt improver.
- Use templates and variables for prompts you reuse (regressions, API contract reviews, test plans).

## 2) Extended Thinking & Transparency

- Ask for private reasoning followed by a concise final answer. Treat the final answer as the source of truth; use the reasoning only for internal review.
- Prefer concise outputs with a checklist of pass/fail criteria (e.g., “history unchanged”, “tie resolved”, “transactional save”).

## 3) Verification & Sanity Pass

- Verify claims about libraries, policies, or best practices against official docs when relevant.
- After a feature is “done,” perform a manual browser pass: create at least one session per scoring mode, confirm leaderboard totals, and ensure historical sessions remain unchanged.

## 4) Context Engineering

- Provide only the minimal, relevant slice of schema, API contract, or rules. Call out success criteria (do not recompute old points) and edge cases (3/4/5 players, ties, guests).
- Include one or two concrete examples (a finished session and the expected leaderboard delta).

## 5) Persistent System Behaviors

- Role: “Assistant for a production web app that tracks board-game sessions; prioritize data integrity, reproducibility, and clear audit trails.”
- Always: use structured sections; list assumptions; highlight uncertainties; propose tests before changes.
- Never: silently change scoring history; ignore tie rules; invent unseen code or APIs.

## 6) Review & QA Checklists

Scoring & Leaderboard

- Raw score preserved? League points assigned exactly once per session?
- Player-count-aware pattern used (descending n..1 by default, or configured)?
- Ties explicitly resolved before save?
- Leaderboard totals = prior totals + this session’s stored points (not recomputed)?
- Session references the exact template version used?

Sessions & Templates

- Session records game, group, playedAt, template (if any), players, placements, points, raw scores, and score details.
- Template fields (labels, multipliers) make sense and are immutable for that saved session.

Data Integrity

- One SessionPlayer per (sessionId, userId); enforce uniqueness.
- Save is atomic; on failure nothing persists.
- Indices support leaderboard and history queries.

UX

- Review step shows placements, points, raw scores, and the point pattern used; user can adjust ties before save.
- History and leaderboard reflect the new session immediately after refresh.

## 7) Prompt-Engineer vs. Redesign

- If issues stem from clarity/format, improve prompts (roles, tags, examples, checklists).
- If issues stem from latency/cost/limits, consider architectural changes rather than more prompting.

## 8) Reusable Prompt Skeletons (Text Only)

A) Architecture/Policy Check

- Role: backend reviewer; goal: confirm persistence-over-recalc; identify risks.
- Inputs: concise schema/contract excerpt, change description, success criteria.
- Output: pass/fail per criterion, risks, and a minimal test plan.

B) Feature Work Plan

- Sections in order: <context>, <requirements>, <edge-cases>, <plan>, <tests>, <risks>, <final>.
- Must include manual browser tests (create session, verify leaderboard delta) before implementation.

C) Tie Policy Resolution

- Provide the exact scoreboard for the session.
- Demand a single explicit tie-break decision before assigning points; reject saving if unresolved.

## 9) Iteration & Knowledge

- For reusable prompts (e.g., scoring review, API contract audit), run them through a prompt improver once, save the improved version, and keep a short rationale in the repo docs.

## 10) Definition of Done (Feature)

- Assumptions and uncertainties listed.
- Subtasks executed in order with clear handoffs.
- Manual browser check completed; leaderboard totals verified by hand for at least one 3‑player and one 4‑player session.
- Short retro: what changed, why it’s safe, how to roll back.
